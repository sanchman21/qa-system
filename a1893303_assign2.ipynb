{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bI98kxTI2LWc",
        "outputId": "84f558fb-1b78-4aeb-8930-e5a2d308cd17"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import ast\n",
        "import collections\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from fastcoref import spacy_component\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import datasets\n",
        "from datasets import load_dataset, load_metric\n",
        "import transformers\n",
        "from transformers import default_data_collator\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from warnings import filterwarnings\n",
        "\n",
        "filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "SC0qa46Gn7Qq"
      },
      "outputs": [],
      "source": [
        "# Set seeds for reproducible results\n",
        "random.seed(420)\n",
        "np.random.seed(420)\n",
        "torch.manual_seed(420)\n",
        "transformers.set_seed(420)\n",
        "\n",
        "MAX_LENGTH = 512 # Maximum number of tokens in a sequence (tokenizer parameter)\n",
        "STRIDE = 128 # Stride when splitting the context into smaller parts (tokenizer parameter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBhcmzFk2LWe"
      },
      "source": [
        "# Coreference Resolution with fastcoref model LingMessCoref"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MW7nfNin2LWg"
      },
      "source": [
        "Coreference resolution helps link expressions that refer to the same entity. Doing this can help a model perform well on information retrieval tasks. Let's create a function that performs coreference resolution and replaces all the expressions with the entities that they refer to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "WE12llYN2LWg",
        "outputId": "2761351e-ac03-47dc-ab22-fd8b4897ab84"
      },
      "outputs": [],
      "source": [
        "if not os.path.isdir(\"./data/squad_resolved\"): # if resolved text data is not saved\n",
        "    # Load the spaCy English language model\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    # Add the fastcoref component to the spaCy pipeline\n",
        "    nlp.add_pipe(\"fastcoref\", config={'model_architecture': 'LingMessCoref', 'model_path': 'biu-nlp/lingmess-coref', 'device': \"cpu\"})\n",
        "\n",
        "    def replace_anaphors_and_cataphors(text):\n",
        "        # Process the input text with spaCy\n",
        "        doc = nlp(text, component_cfg={\"fastcoref\": {'resolve_text': True}})\n",
        "        return doc._.resolved_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30kZwwqQ2LWh"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3wmxz0F2LWh"
      },
      "source": [
        "We'll use the SQUAD dataset to train our model since it is an annotated dataset. Information Retrieval is the task of retrieving information from a document. Using SQUAD, we'll perform extractive question answering which is a form of informationr retrieval since we have to identify the span of text in the passage that answers a question related to the passage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GFKaTtJBbV_"
      },
      "source": [
        "Squad v1.1 has a lot of rows. Even with a GPU using colab free, it is difficult to resolve the text using coreference resolution and train on all the rows. Let's take every 7th row for both the train and validation datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Coreference resolution is performed on the squad subset and saved so that there is no need to resolve the text everytime this script is run. The saved data can be loaded straightaway. It is better to save the unresolved squad as well and load it directly. Due to version errors and library issues, the dataset is saved as a csv dataframe using pandas instead of using datasets.save_to_disk(). Errors were encountered while using datasets.load_from_disk() and the dataset wouldn't load."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "do_coref = False\n",
        "\n",
        "# if the data is already preprocessed and saved, load it\n",
        "if os.path.isdir(\"./data/squad_resolved\") and os.path.isdir(\"./data/squad_unresolved\"):\n",
        "    if do_coref: # if resolved data is needed\n",
        "        train_df = pd.read_csv(\"./data/squad_resolved/train_resolved.csv\") # load the train dataset\n",
        "        val_df = pd.read_csv(\"./data/squad_resolved/val_resolved.csv\") # load the val dataset\n",
        "        for idx in range(len(train_df[\"answers\"])): # for all answers\n",
        "            # convert answers type from string to dict\n",
        "            train_df[\"answers\"][idx] = ast.literal_eval(train_df[\"answers\"][idx])\n",
        "        for idx in range(len(val_df[\"answers\"])): # for all answers\n",
        "            # convert answers type from string to dict\n",
        "            val_df[\"answers\"][idx] = ast.literal_eval(val_df[\"answers\"][idx])\n",
        "        squad = datasets.DatasetDict(\n",
        "            {\n",
        "                \"train\": datasets.Dataset.from_pandas(train_df), # train dataset\n",
        "                \"validation\": datasets.Dataset.from_pandas(val_df), # val dataset\n",
        "            }\n",
        "        )\n",
        "    else: # load the unresolved dataaset \n",
        "        train_df = pd.read_csv(\"./data/squad_unresolved/train_unresolved.csv\") # load the train dataset\n",
        "        val_df = pd.read_csv(\"./data/squad_unresolved/val_unresolved.csv\") # load the val dataset\n",
        "        for idx in range(len(train_df[\"answers\"])): # for all answers \n",
        "            # convert answers type from string to dict\n",
        "            train_df[\"answers\"][idx] = ast.literal_eval(train_df[\"answers\"][idx])\n",
        "        for idx in range(len(val_df[\"answers\"])): # for all answers\n",
        "            # convert answers type from string to dict\n",
        "            val_df[\"answers\"][idx] = ast.literal_eval(val_df[\"answers\"][idx])\n",
        "        squad = datasets.DatasetDict( # create a dataset dict\n",
        "            {\n",
        "                \"train\": datasets.Dataset.from_pandas(train_df), # train dataset\n",
        "                \"validation\": datasets.Dataset.from_pandas(val_df), # val dataset\n",
        "            }\n",
        "        )\n",
        "else:  # save datasets \n",
        "    squad = load_dataset(\"squad\") # load squad dataset\n",
        "    # shuffle and select every 7th row for both train and val splits\n",
        "    squad[\"train\"]= squad[\"train\"].shuffle(seed=420).select(range(0, squad[\"train\"].num_rows, 7))\n",
        "    squad[\"val\"] = squad[\"val\"].shuffle(seed=420).select(range(0, squad[\"val\"].num_rows, 7))\n",
        "    train_unresolved = pd.DataFrame(squad[\"train\"]) # create a dataframe from the train dataset (unresolved)\n",
        "    val_unresolved = pd.DataFrame(squad[\"validation\"]) # create a dataframe from the val dataset (unresolved)\n",
        "    train_unresolved.to_csv(\"./data/squad_unresolved/train_unresolved.csv\", index=False) # save the train dataset (unresolved)\n",
        "    val_unresolved.to_csv(\"./data/squad_unresolved/val_unresolved.csv\", index=False) # save the val dataset (unresolved)\n",
        "\n",
        "    if do_coref: # if text needs to be resolved\n",
        "        for i in tqdm(range(len(squad[\"train\"]))): # for each context in the train dataset\n",
        "            # resolve text\n",
        "            squad[\"train\"][i][\"context\"] = replace_anaphors_and_cataphors(squad[\"train\"][i][\"context\"])\n",
        "\n",
        "        for i in tqdm(range(len(squad[\"validation\"]))): # for each context in the val dataset\n",
        "            # resolve text\n",
        "            squad[\"validation\"][i][\"context\"] = replace_anaphors_and_cataphors(squad[\"validation\"][i][\"context\"])\n",
        "\n",
        "        train_resolved = pd.DataFrame(squad[\"train\"]) # create a dataframe from the train dataset (resolved)\n",
        "        val_resolved = pd.DataFrame(squad[\"validation\"]) # create a dataframe from the val dataset (resolved)\n",
        "        train_resolved.to_csv(\"./data/squad_resolved/train_resolved.csv\", index=False) # save the train dataset (resolved)\n",
        "        val_resolved.to_csv(\"./data/squad_resolved/val_resolved.csv\", index=False) # save the val dataset (resolved)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BDS7iFhB3L9"
      },
      "source": [
        "Let's take a look at a random example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RyTe6WFd2LWi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here is a random SQuAD training set example (row number = 431): \n",
            "Freemasonry, as it exists in various forms all over the world, has a membership estimated by the United Grand Lodge of England at around six million worldwide. The fraternity is administratively organised into independent Grand Lodges (or sometimes Grand Orients), each of which governs its own Masonic jurisdiction, which consists of subordinate (or constituent) Lodges. The largest single jurisdiction, in terms of membership, is the United Grand Lodge of England (with a membership estimated at around a quarter million). The Grand Lodge of Scotland and Grand Lodge of Ireland (taken together) have approximately 150,000 members. In the United States total membership is just under two million.\n",
            "How many members does the Grand Lodge of Scotland and the Grand Lodge of Ireland have, together?\n",
            "{'answer_start': [616], 'text': ['150,000']}\n"
          ]
        }
      ],
      "source": [
        "n = random.randint(0, squad[\"train\"].num_rows)\n",
        "print(f\"Here is a random SQuAD training set example (row number = {n}): \")\n",
        "print(squad[\"train\"][n][\"context\"])\n",
        "print(squad[\"train\"][n][\"question\"])\n",
        "print(squad[\"train\"][n][\"answers\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Training and Evaluation\n",
        "\n",
        "Skip this part if you've already trained and evaluated the model. Jump ahead to the \"System Testing\" section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZYBzYAw2LWi"
      },
      "source": [
        "Now let's load the pre-trained model and tokenizer from huggingface. RoBERTa is used instead of BERT since RoBERTa is an optimized version of BERT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "xGbbYR0C2LWj"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"FacebookAI/roberta-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_9vJtY752LWj"
      },
      "outputs": [],
      "source": [
        "def prepare_train_features(sample, max_length = MAX_LENGTH, stride = STRIDE):\n",
        "    # some questions have leading spaces which need to be removed\n",
        "    sample[\"question\"] = [q.lstrip() for q in sample[\"question\"]]\n",
        "    # Tokenize the sample with truncation and padding, but keep the overflows using a stride.\n",
        "    # When a context is long, it will be chunked into several features with an overlap of 128 tokens.\n",
        "    tokenized_sample = tokenizer(\n",
        "        sample[\"question\"],\n",
        "        sample[\"context\"],\n",
        "        truncation = \"only_second\",  # truncate context, not the question\n",
        "        max_length=max_length, # max length of the text that can go to the model\n",
        "        stride=stride, # overlap of 128 tokens when a context is chunked into several features\n",
        "        return_overflowing_tokens=True, # return all the chunks if the context is long\n",
        "        return_offsets_mapping=True, # return the mapping between the tokens and the character positions\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
        "    # its corresponding example. This key gives us just that.\n",
        "    sample_mapping = tokenized_sample.pop(\"overflow_to_sample_mapping\")\n",
        "    # The offset mappings will give us a map from token to character position in the original context.\n",
        "    # This will help us compute the start_positions and end_positions.\n",
        "    offset_mapping = tokenized_sample.pop(\"offset_mapping\")\n",
        "\n",
        "    # Let's label those examples!\n",
        "    tokenized_sample[\"start_positions\"] = []\n",
        "    tokenized_sample[\"end_positions\"] = []\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        input_ids = tokenized_sample[\"input_ids\"][i] # get the input_ids\n",
        "\n",
        "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "        sequence_ids = tokenized_sample.sequence_ids(i)\n",
        "\n",
        "        # One example can give several spans, this is the index of the example containing this span of text.\n",
        "        sample_index = sample_mapping[i]\n",
        "        answers = sample[\"answers\"][sample_index]\n",
        "\n",
        "        # Start/end character index of the answer in the text.\n",
        "        start_char = answers[\"answer_start\"][0]\n",
        "        end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "        # Start token index of the current span in the text.\n",
        "        token_start_index = 0\n",
        "        while sequence_ids[token_start_index] != 1:\n",
        "            token_start_index += 1\n",
        "\n",
        "        # End token index of the current span in the text.\n",
        "        token_end_index = len(input_ids) - 1\n",
        "        while sequence_ids[token_end_index] != 1:\n",
        "            token_end_index -= 1\n",
        "\n",
        "        # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
        "        while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "            token_start_index += 1\n",
        "        tokenized_sample[\"start_positions\"].append(token_start_index - 1)\n",
        "        while offsets[token_end_index][1] >= end_char:\n",
        "            token_end_index -= 1\n",
        "        tokenized_sample[\"end_positions\"].append(token_end_index + 1)\n",
        "\n",
        "    return tokenized_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "_v3cRDER2LWk"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 12515/12515 [00:03<00:00, 3724.86 examples/s]\n",
            "Map: 100%|██████████| 1510/1510 [00:00<00:00, 3763.96 examples/s]\n"
          ]
        }
      ],
      "source": [
        "tokenized_datasets = squad.map(prepare_train_features, batched=True,\n",
        "                               remove_columns=squad[\"train\"].column_names) # tokenize datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Y3aAvq0o2LWk"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available(): # if cuda is available\n",
        "    torch.cuda.set_device(0) # set device to cuda\n",
        "\n",
        "model_name = model_checkpoint.split(\"/\")[-1] # set model name\n",
        "batch_size = 4 # set batch size\n",
        "training_args = TrainingArguments( # set training arguments\n",
        "    f\"{model_name}-finetuned-squad-coref-{do_coref}\", # set output directory name\n",
        "    evaluation_strategy = \"epoch\", # evaluate after each epoch\n",
        "    learning_rate=2e-5, # set learning rate\n",
        "    gradient_accumulation_steps=4, # set gradient accumulation steps\n",
        "    per_device_train_batch_size=batch_size, # set batch size\n",
        "    per_device_eval_batch_size=batch_size, # set batch size\n",
        "    num_train_epochs=3, # set number of epochs\n",
        "    weight_decay=0.01, # set weight decay\n",
        "    push_to_hub=False, # do not push to hub\n",
        "    disable_tqdm=False, # enable tqdm\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "mudFOpxh2LWk"
      },
      "outputs": [],
      "source": [
        "# define data collator\n",
        "# Data collator converts the dataset into a batch of tensors\n",
        "data_collator = default_data_collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "3GbmaR3h2LWl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mps\n"
          ]
        }
      ],
      "source": [
        "trainer = Trainer( # create a trainer\n",
        "    model, # pass the model\n",
        "    training_args, # pass the training arguments\n",
        "    train_dataset=tokenized_datasets[\"train\"], # pass the train dataset\n",
        "    eval_dataset=tokenized_datasets[\"validation\"], # pass the validation dataset\n",
        "    data_collator=data_collator, # pass the data collator\n",
        "    tokenizer=tokenizer, # pass the tokenizer\n",
        ")\n",
        "\n",
        "print(trainer.args.device) # print trainer device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "wra1bIr12LWl",
        "outputId": "58d2d8fe-dc02-4e1c-c48b-5ec0a799b799"
      },
      "outputs": [],
      "source": [
        "trainer.train() # train using trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vngb1lmpge6K"
      },
      "outputs": [],
      "source": [
        "if do_coref: # if resolved text is used\n",
        "    trainer.save_model(\"./roberta-squad-resolved\") # save the resolved text model\n",
        "else: # if unresolved text is used\n",
        "    trainer.save_model(\"./roberta-squad-unresolved\") # save the unresolved text model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Please delete any other directory created by the trainer since that will contain models trained in each epoch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The validation dataset needs some processing since we need to map the output logits to the start and end positions in text and further to the predicted answer text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "if do_coref: # if resolved text is used\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"./roberta-squad-resolved\") # load the resolved text tokenizer\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(\"./roberta-squad-resolved\") # load the resolved text model\n",
        "    trainer = Trainer(model=model, tokenizer=tokenizer) # create a trainer\n",
        "else: # if unresolved text is used\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"./roberta-squad-unresolved\") # load the unresolved text tokenizer\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(\"./roberta-squad-unresolved\") # load the unresolved text model\n",
        "    trainer = Trainer(model=model, tokenizer=tokenizer) # create a trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_validation_features(examples, max_length = MAX_LENGTH, stride = STRIDE):\n",
        "    # some questions have whitespace on the left which needs to be removed\n",
        "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
        "\n",
        "    # tokenize the examples in the same way as the training set\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"question\"], # get question\n",
        "        examples[\"context\"], # get context\n",
        "        truncation=\"only_second\", # truncate context, not the question\n",
        "        max_length=max_length, # max length of the text that can go to the model\n",
        "        stride=stride, # overlap of 128 tokens when a context is chunked into several features\n",
        "        return_overflowing_tokens=True, # return all the chunks if the context is long\n",
        "        return_offsets_mapping=True, # return the mapping between the tokens and the character positions\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
        "    # its corresponding example. This key gives us just that.\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "    # We keep the example_id that gave us this feature and we will store the offset mappings.\n",
        "    tokenized_examples[\"example_id\"] = []\n",
        "\n",
        "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
        "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "        context_index = 1\n",
        "\n",
        "        # One example can give several spans, this is the index of the example containing this span of text.\n",
        "        sample_index = sample_mapping[i]\n",
        "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
        "\n",
        "        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
        "        # position is part of the context or not.\n",
        "        tokenized_examples[\"offset_mapping\"][i] = [\n",
        "            (o if sequence_ids[k] == context_index else None)\n",
        "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
        "        ]\n",
        "\n",
        "    return tokenized_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 1510/1510 [00:00<00:00, 2892.37 examples/s]\n"
          ]
        }
      ],
      "source": [
        "validation_features = squad[\"validation\"].map(\n",
        "    prepare_validation_features,\n",
        "    batched=True,\n",
        "    remove_columns=squad[\"validation\"].column_names\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 190/190 [01:32<00:00,  2.05it/s]\n"
          ]
        }
      ],
      "source": [
        "output = trainer.predict(validation_features) # get predictions\n",
        "# set back hidden columns\n",
        "validation_features.set_format(type=validation_features.format[\"type\"], \n",
        "                               columns=list(validation_features.features.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# map examples with corresponding features\n",
        "examples = squad[\"validation\"]\n",
        "features = validation_features\n",
        "\n",
        "example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "features_per_example = collections.defaultdict(list)\n",
        "for i, feature in enumerate(features):\n",
        "    features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "n_best_size will be used to get all possible answers from different start and end positions since the best answer can be a wrong prediction (a span present in the question or outside the context). max_answer_length will eliminate extremely long answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n",
        "    all_start_logits, all_end_logits = raw_predictions\n",
        "    # Build a map example to its corresponding features.\n",
        "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "    features_per_example = collections.defaultdict(list)\n",
        "    for i, feature in enumerate(features):\n",
        "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
        "\n",
        "    # The dictionaries we have to fill.\n",
        "    predictions = collections.OrderedDict()\n",
        "\n",
        "    # Logging.\n",
        "    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n",
        "\n",
        "    # Let's loop over all the examples!\n",
        "    for example_index, example in enumerate(tqdm(examples)):\n",
        "        # Those are the indices of the features associated to the current example.\n",
        "        feature_indices = features_per_example[example_index]\n",
        "\n",
        "        min_null_score = None # Only used if squad_v2 is True.\n",
        "        valid_answers = []\n",
        "        \n",
        "        context = example[\"context\"]\n",
        "        # Looping through all the features associated to the current example.\n",
        "        for feature_index in feature_indices:\n",
        "            # We grab the predictions of the model for this feature.\n",
        "            start_logits = all_start_logits[feature_index]\n",
        "            end_logits = all_end_logits[feature_index]\n",
        "            # This is what will allow us to map some the positions in our logits to span of texts in the original\n",
        "            # context.\n",
        "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
        "\n",
        "            # Update minimum null prediction.\n",
        "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
        "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
        "            if min_null_score is None or min_null_score < feature_null_score:\n",
        "                min_null_score = feature_null_score\n",
        "\n",
        "            # Go through all possibilities for the `n_best_size` greater start and end logits.\n",
        "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
        "                    # to part of the input_ids that are not in the context.\n",
        "                    if (\n",
        "                        start_index >= len(offset_mapping)\n",
        "                        or end_index >= len(offset_mapping)\n",
        "                        or offset_mapping[start_index] is None\n",
        "                        or offset_mapping[end_index] is None\n",
        "                    ):\n",
        "                        continue\n",
        "                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
        "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
        "                        continue\n",
        "\n",
        "                    start_char = offset_mapping[start_index][0]\n",
        "                    end_char = offset_mapping[end_index][1]\n",
        "                    valid_answers.append(\n",
        "                        {\n",
        "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                            \"text\": context[start_char: end_char]\n",
        "                        }\n",
        "                    )\n",
        "        \n",
        "        if len(valid_answers) > 0:\n",
        "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
        "        else:\n",
        "            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
        "            # failure.\n",
        "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
        "        \n",
        "        # Let's pick the final answer\n",
        "        predictions[example[\"id\"]] = best_answer[\"text\"]\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Post-processing 1510 example predictions split into 1517 features.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1510/1510 [00:02<00:00, 677.23it/s]\n"
          ]
        }
      ],
      "source": [
        "final_predictions = postprocess_qa_predictions(squad[\"validation\"], validation_features, output.predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "metric = load_metric(\"squad\") # load squad metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'exact_match': 81.52317880794702, 'f1': 89.27663783927457}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
        "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in squad[\"validation\"]]\n",
        "metric.compute(predictions=formatted_predictions, references=references)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 12515\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 1510\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "squad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# System Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "03/30/2024 11:26:09 - INFO - \t missing_keys: []\n",
            "03/30/2024 11:26:09 - INFO - \t unexpected_keys: []\n",
            "03/30/2024 11:26:09 - INFO - \t mismatched_keys: []\n",
            "03/30/2024 11:26:09 - INFO - \t error_msgs: []\n",
            "03/30/2024 11:26:09 - INFO - \t Model Parameters: 590.0M, Transformer: 434.6M, Coref head: 155.4M\n"
          ]
        }
      ],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Add the fastcoref component to the spaCy pipeline\n",
        "nlp.add_pipe(\"fastcoref\", config={'model_architecture': 'LingMessCoref', 'model_path': 'biu-nlp/lingmess-coref', 'device': \"cpu\"})\n",
        "\n",
        "def replace_anaphors_and_cataphors(text):\n",
        "    # Process the input text with spaCy\n",
        "    doc = nlp(text, component_cfg={\"fastcoref\": {'resolve_text': True}})\n",
        "    return doc._.resolved_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model and tokenizer loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "is_finetuned = input(\"Selected fine-tuned model? (Y if model is fine-tuned using the code above, N if not)\")\n",
        "is_finetuned = True if is_finetuned.lower() == \"y\" else False\n",
        "if is_finetuned:\n",
        "    is_coref = input(\"With coreference resolution? (Y if you want to resolve the text, N if not)\")\n",
        "    is_coref = True if is_coref.lower() == \"y\" else False\n",
        "    if is_coref:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"./roberta-squad-resolved\")\n",
        "        model = AutoModelForQuestionAnswering.from_pretrained(\"./roberta-squad-resolved\")\n",
        "    else:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"./roberta-squad-unresolved\")\n",
        "        model = AutoModelForQuestionAnswering.from_pretrained(\"./roberta-squad-unresolved\")\n",
        "else:\n",
        "    is_coref = False\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(\"FacebookAI/roberta-base\")\n",
        "\n",
        "model.to(\"cpu\")\n",
        "print(\"Model and tokenizer loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "03/30/2024 11:27:24 - INFO - \t Tokenize 1 inputs...\n",
            "Map: 100%|██████████| 1/1 [00:00<00:00, 80.31 examples/s]\n",
            "03/30/2024 11:27:24 - INFO - \t ***** Running Inference on 1 texts *****\n",
            "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'using the primary elements of theater such as scenery, costumes, and acting'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "context = \"\"\"Opera refers to a dramatic art form, originating in Europe, in which the \n",
        "emotional content is conveyed to the audience as much through music, both vocal and instrumental, \n",
        "as it is through the lyrics. By contrast, in musical theater an actor's dramatic performance \n",
        "is primary, and the music plays a lesser role. The drama in opera is presented using the \n",
        "primary elements of theater such as scenery, costumes, and acting. However, the words of the \n",
        "opera, or libretto, are sung rather than spoken. The singers are accompanied by a musical \n",
        "ensemble ranging from a small instrumental ensemble to a full symphonic orchestra.\"\"\"\n",
        "\n",
        "question = \"How is drama presented in Opera?\"\n",
        "\n",
        "change = input(\"Do you want to input your own context and question or continue with the sample provided? (Y if you want to input your own context and question, N if not)\")\n",
        "change = True if change.lower() == \"y\" else False\n",
        "if change:\n",
        "    context = input(\"Enter the context: \")\n",
        "    question = input(\"Enter the question: \")\n",
        "\n",
        "if is_coref:\n",
        "    context = replace_anaphors_and_cataphors(context)\n",
        "\n",
        "inputs = tokenizer(question, context, return_tensors=\"pt\").to(\"cpu\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(inputs.input_ids, attention_mask=inputs.attention_mask)\n",
        "\n",
        "answer_start_index = outputs.start_logits.argmax()\n",
        "answer_end_index = outputs.end_logits.argmax()\n",
        "\n",
        "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
        "tokenizer.decode(predict_answer_tokens, skip_special_tokens=True).lstrip().replace(\"\\n\", \"\") # clean answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To test all the combinations, simply re-run the last 2 code cells and specify the configuration you want.. After testing all the combinations (pretrained roberta, finetuned roberta with unresolved text, finetuned roberta with resolved text), you'll see the only the finetuned combinations give the right answer."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
